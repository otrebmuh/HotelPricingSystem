# **Evaluation of the Hotel Pricing System Architecture**

**Table 1: Architectural Drivers Coverage** – The table below summarizes each user story (HPS-\#) and quality attribute scenario (QA-\#), the iteration in which it’s addressed, the key architectural solutions for it, and an assessment of the design’s adequacy.

| Driver (User Story or QA) | Iteration | Architectural Solution (Key Components/Decisions) | Adequacy Evaluation (Design Sufficiency) |
| ----- | ----- | ----- | ----- |
| **HPS-1: Log In** – User authentication | 5 | **Auth Service \+ SSO** – Dedicated Authentication Service integrates with cloud Identity provider (OAuth 2.0/OIDC) for login. Uses JWT tokens and role-based access control (RBAC) to restrict functionality per user. | **Adequate:** Leverages industry-standard SSO and centralized auth. Ensures secure login and proper authorization scoping, aligning with security best practices. |
| **HPS-2: Change Prices** – Core pricing | 2 | **Price Management Service (CQRS)** – A Price Management microservice handles price updates (write side), using event sourcing to record every change and an outbox to reliably publish events. In-memory caching accelerates calculations, and events propagate changes to other systems (CMS) via an Event Bus with circuit breakers for fault isolation. | **Adequate:** The design meets performance (\<100ms calc) via caching and optimized logic, and ensures **100% reliable publication** of price updates using an event store \+ outbox (no lost updates). It cleanly decouples price calculations from distribution, though the event-driven approach adds complexity (manageable given the reliability gains). |
| **HPS-3: Query Prices** – Price lookup | 3 | **Price Query Service (Read-optimized)** – A separate Query service (CQRS read side) with its own denormalized database is used for fast price lookups. It employs distributed caching and can scale horizontally to handle high query volume. Multi-region deployment and load balancing provide failover for 99.9% uptime. | **Adequate:** By isolating read operations and optimizing data for queries, the system can handle up to 1M daily queries with minimal latency impact. The multi-region, scalable design supports the 99.9% availability SLA, albeit with added operational complexity (e.g. data replication delays). |
| **HPS-4: Manage Hotels** – Admin hotel info mgmt. | 4 | **Hotel Service (DDD \+ Events)** – A dedicated Hotel Management service encapsulates hotel data and business rules using domain-driven design. It validates inputs and uses optimistic concurrency to prevent conflicts. Successful changes trigger hotel events on the Event Bus to synchronize other services. | **Adequate:** The design cleanly supports adding/updating hotels with proper validation and **data integrity**. Publishing hotel changes as events keeps data consistent across microservices. The approach is robust for current needs, though DDD and concurrency control introduce some complexity that the team must manage. |
| **HPS-5: Manage Rates** – Admin rate & rule mgmt. | 4 | **Rate Service \+ Rule Engine** – A Rate Management service handles rate definitions and business rules. It includes a **rule engine** to define calculation formulas for derived rates, allowing flexible updates without code changes. An in-app test sandbox lets users validate new pricing rules before applying them. Rate changes are published as events to update dependent pricing components. | **Adequate:** The built-in rule engine provides needed flexibility for evolving pricing strategies, and the test environment reduces errors by catching rule issues early. This design meets the rate management needs effectively, with acceptable added complexity (the rule engine integration and maintenance). |
| **HPS-6: Manage Users** – Admin user permissions | 5 | **Centralized Authorization** – User account auth is handled by the cloud IdP, while the HPS maintains its own authorization store for permissions/roles. An Admin Portal interface allows centralized role/permission management via the Auth service. All services rely on this central RBAC policy (enforced at the API Gateway and services). | **Adequate:** Offloading authentication to a proven cloud service and centralizing permission control ensures **consistent security** across the system. The solution covers user-management functionality (assigning roles, etc.) well, though integration with the external IdP must be carefully managed to avoid sync issues. |
| **QA-1: Performance** (Pricing calc \<100 ms) | 2 | **Optimized Calc Engine \+ Caching** – The Price Calculation component is built for speed, using precomputed business rules and caching to avoid expensive operations. CQRS separation ensures the write service does minimal work (only computing and emitting events), with read workloads offloaded to the Query service. | **Adequate:** The architecture explicitly targets sub-100 ms price updates (the calc engine is designed to meet this SLA, even using cached data to accelerate computations). By minimizing database hits and segregating reads, the design should achieve the performance goal. As long as caching and logic are tuned, this requirement is well-handled. |
| **QA-2: Reliability** (100% price updates published) | 2 | **Reliable Eventing (Event Sourcing \+ Outbox)** – Every price change is stored as an event in a persistent log (event store) to maintain a history and enable recovery. An outbox mechanism atomically records events with DB changes and guarantees delivery to the Event Bus (exactly-once publishing). **Circuit breakers** wrap external calls (e.g. to CMS) to prevent cascade failures, and batch operations are handled transactionally to avoid partial failures. | **Adequate:** The combination of event sourcing and outbox ensures no update is lost – every change can be replayed or audited, and is eventually delivered to downstream systems. This design should fulfill the 100% publication reliability. It adds complexity (distributed events, eventual consistency), but greatly improves resilience (e.g. the system can queue updates if the CMS is down, then catch up safely). |
| **QA-3: Availability** (99.9% uptime for queries) | 3 | **Fault-Tolerant Deployment** – The Query service (and other critical components) can be deployed in multiple regions with load balancing and automated failover. The system is containerized and orchestrated to allow quick replacement of failed instances. Consistent monitoring and health checks facilitate rapid detection and failover. Data is replicated to ensure query nodes in each region have up-to-date prices (with eventual consistency). | **Adequate:** A multi-region, highly redundant deployment directly supports the 99.9% uptime goal. If one region or instance fails, queries are routed to a healthy instance, minimizing downtime. This approach meets the availability requirement, though careful implementation of data replication and failover logic is needed to avoid inconsistencies during regional outages. |
| **QA-4: Scalability** (100k–1M queries/day) | 3 | **Horizontal Scaling \+ Caching** – The stateless Query service can be replicated horizontally to handle increasing load. A distributed in-memory cache serves frequent queries to drastically reduce database load. The read data model is optimized (denormalized) for quick retrieval without heavy joins. Rate limiting is enforced at the API Gateway to prevent any single client from overwhelming the system. | **Adequate:** The system can scale out simply by adding more query service instances (and cache nodes), which addresses the target throughput with headroom. The use of caching and a read-optimized store means even a 10× increase in query volume should be supported with only modest impact on latency. As a result, the scalability requirements are well covered by the design. |
| **QA-5: Security** (Secure login & authz) | 5 | **End-to-End Security Layer** – The design uses **OAuth2 \+ OpenID Connect** with the cloud identity provider for robust authentication. It issues JWT tokens for stateless, scalable session management. Fine-grained authorization is implemented via RBAC (roles determine accessible functions) enforced in the API Gateway and services. Additionally, all security-sensitive actions are logged for audit and intrusion detection. | **Adequate:** By utilizing standard, **proven security protocols** and centralized enforcement, the architecture meets security needs for authentication and access control. Users only see and execute authorized functions, satisfying the scenario. The comprehensive audit logging further strengthens security. The approach is robust, though it relies on correct configuration of roles and careful management of tokens. |
| **QA-6: Modifiability** (Add gRPC API easily) | 6 | **Protocol-Independent Architecture** – Services are designed with a layer of protocol adapters so that new interfaces (e.g. a gRPC endpoint) can plug in without altering core business logic. The system uses technology-neutral data contracts (e.g. Protocol Buffers) to allow exposing gRPC in parallel with REST. The API Gateway also can translate/route different protocols uniformly. | **Adequate:** The core logic is separated from API layers, meaning a new gRPC interface can be added by implementing new controllers or adapters with no change to the underlying services. This achieves the modifiability scenario – the design is flexible to extension. Proper abstraction of protocol details (already planned) will be key to realizing this benefit fully. |
| **QA-7: Deployability** (Move between envs) | 1 | **CI/CD & Containerization** – The architecture emphasizes using Infrastructure as Code and containerized deployments for all environments. A continuous integration/continuous deployment pipeline automates promotion from dev to integration to staging to prod, using the same container images and configurations per environment. External configurations (e.g. environment-specific settings) are separated from code. | **Adequate:** With reproducible containerized environments and automated deployments, the system can be deployed across dev/test/staging/production with **no code changes**, satisfying the deployability requirement. This approach reduces configuration drift and human error. It assumes the team sets up thorough automation and uses config management diligently, which appears to be in place. |
| **QA-8: Monitorability** (Collect 100% metrics) | 6 | **Unified Monitoring & Telemetry** – All services are instrumented using a standard observability framework (OpenTelemetry) to capture metrics, traces, and logs for all price update operations. A centralized monitoring infrastructure aggregates these for real-time dashboards in the Admin Portal and alerts. Correlation IDs in logs enable end-to-end tracing of requests. | **Adequate:** The design provides **full-stack visibility** into system behavior, which means performance and reliability metrics (e.g. for price publications) can be gathered comprehensively. This meets the monitorability scenario. The use of an industry-standard telemetry solution ensures completeness and consistency of monitoring data across services. |
| **QA-9: Testability** (Test independent of externals) | 6 | **Decoupled Test Environment** – External dependencies (PMS, CMS, etc.) are abstracted behind APIs/event bus, allowing the use of mocks or containerized test instances in integration testing. The team employs testcontainers to spin up ephemeral services/databases for automated integration tests. Contract testing between services is planned to catch incompatibilities early. | **Adequate:** The architecture’s modular, decoupled nature and use of containerized test setups means the entire system can be tested in isolation from real external systems. This fulfills the testability requirement by enabling reliable integration tests (and each microservice can be tested independently as well). The approach should yield high confidence in releases, provided the mocks and test containers are kept in sync with actual systems. |

## **Key Design Decisions for Each Driver**

The proposed architecture makes explicit design choices to address each requirement. Below we detail the key design decisions associated with each user story and quality attribute driver:

### **User Stories (Functional Drivers)**

* **HPS-1 (Log In):** The system uses a dedicated **Authentication Service** that delegates user credential validation to a cloud identity provider via **OAuth 2.0/OpenID Connect**. This means the HPS itself does not store passwords – instead, it relies on Single Sign-On and receives a token upon successful login. The Auth Service and API Gateway enforce that only authenticated users with valid JWT tokens can access the system, and the UI will show/hide functionality based on the user’s roles. This design decision ensures a secure and centralized login mechanism aligned with enterprise practices, leveraging the cloud provider’s user store (per **CON-2**) and avoiding a custom auth implementation.

* **HPS-2 (Change Prices):** The architecture centers on a **Price Management Service** to handle price changes. This service follows a **CQRS pattern**, focusing only on write operations (applying price updates and business rules). It incorporates an in-memory **Price Calculation Engine** to compute derived prices quickly, using cached data for performance. To ensure every change is reliably recorded and propagated (addressing reliability QA-2), the design uses **event sourcing** – every price change is stored as an event in a **Price Event Store** (event log). An **Event Publisher with an Outbox** then ensures those events are delivered to other components and external systems exactly once. This asynchronous event-driven approach, combined with a **Circuit Breaker** on the integration to the Channel Management System, guarantees that price updates will eventually reach all consumers without loss, even if external services are temporarily unavailable. In summary, the design for HPS-2 emphasizes fast computations (to meet performance QA-1) and a reliable publication mechanism (to meet reliability QA-2) for all price changes.

* **HPS-3 (Query Prices):** To handle price inquiries efficiently, the design introduces a separate **Price Query Service** dedicated to read operations. This service maintains its own **query-optimized database** (a denormalized read model) so that retrieving prices is very fast and not dependent on the transactional schema used for updates. The Query Service also uses a **Distributed Cache** to serve frequent queries from memory, further reducing response times and load on the database. The system can spawn multiple instances of the Query Service behind the API Gateway (horizontal scaling) to handle high request volumes (scalability QA-4). To achieve the 99.9% uptime requirement (availability QA-3), the architecture allows deploying the query services in **multiple regions**, with a load balancer or DNS-based routing to fail over if one region goes down. The Query Service subscribes to price change events (from HPS-2) via the Event Bus so that its read database is updated whenever prices change, ensuring that end-users and external systems always query the latest prices (with minimal delay). This read-side decoupling is a strategic decision to meet heavy query load and uptime demands without impacting the price update functionality.

* **HPS-4 (Manage Hotels):** The design allocates hotel management to its own **Hotel Management Service**, reflecting a microservice separation of concerns. This service is built using **Domain-Driven Design (DDD)** principles – it encapsulates hotel-related business rules (tax rates, room types, etc.) within a rich domain model. The service exposes APIs for creating or updating hotels, with rigorous **input validation** and an **optimistic concurrency control** mechanism to handle concurrent admin changes without locking. A key design decision is that after a successful hotel update, the service will **publish an event** (e.g. HotelUpdated) to the Event Bus. Other services (like Pricing or Reservation systems) can listen for these events to stay in sync with hotel data changes. This event-driven update propagation ensures data consistency across the system and reduces tight coupling. In summary, for HPS-4 the architecture emphasizes maintainability and integrity: a clear separation of hotel domain logic (via DDD) and a reliable way to broadcast changes to interested parts of the system.

* **HPS-5 (Manage Rates):** The Rate Management functionality is handled by a **Rate Service**. Recognizing that rate calculation logic can vary and evolve, the architecture includes a pluggable **Rule Engine** component inside this service. This allows administrators to define or modify the business rules (formulas, discount structures, etc.) for deriving various rate plans from a base price without changing code. The Rule Engine approach makes the system flexible to new pricing strategies. Additionally, the design provides a **test execution environment (sandbox)** for rate rules. An admin can simulate and validate a new or modified rule to see its effects before deploying it to production, which addresses potential errors proactively. Like other admin actions, changes in rates trigger events (RateCreated or RateUpdated) that are published to notify the Pricing service to possibly recalculate prices if needed. The key decisions here – using a rule engine and sandbox – directly support modifiability and correctness for complex pricing logic.

* **HPS-6 (Manage Users):** User management is implemented in coordination with the external identity system. The design decision is to **centralize permission management** within the HPS environment, while actual user authentication and basic profile data remain in the cloud identity service. In practice, this means administrators (via the Admin Portal) can assign application-specific roles or permissions to users (who are defined in the external IdP). The **Auth Service** stores and enforces these authorization rules. This approach prevents inconsistencies (all services consult one auth source for authorizations) and simplifies auditing changes to user permissions. It also adheres to **CON-2** by leveraging the cloud provider for identity management but still gives HPS control over authorization. A specific design element is that any changes to user roles could be captured in audit logs (security logging) and, if complex enough, even propagated as an event if other systems need to know about user permission changes (though primarily, it’s an internal concern). Overall, the architecture for HPS-6 ensures that administrators have a single, consistent interface to manage user access rights, which then uniformly take effect across the system (through the API Gateway and Auth Service enforcement).

### **Quality Attribute Scenarios (Non-Functional Drivers)**

* **QA-1 (Performance – price calc \<100 ms):** The 100 ms update requirement guided several design choices. The **Price Calculation Engine** in the Price Service is optimized to be as efficient as possible – it loads any necessary reference data (business rules, recent prices) in memory and uses **cache-aside** patterns to avoid repetitive database reads. By using CQRS, the system separates the read workload entirely, meaning the write side (which performs the calculation when a price is changed) can dedicate all its resources to computation and event recording without contending with heavy query traffic. The architecture also limits synchronous external calls in the price-change pathway: for example, instead of calling the Channel Management System in-line for each price change, the design publishes to an event/outbox so that external notifications can happen asynchronously. This prevents external system latency from slowing down the user’s price update operation. In essence, **pre-computation, caching, and asynchronous processing** are the key tactics employed to meet the strict performance target of QA-1.

* **QA-2 (Reliability – 100% updates published):** To ensure every price change is applied and disseminated reliably, the architecture employs an **event-driven, transactionally-safe update mechanism**. When a price is changed, the system writes to its database and an **event log (Event Store)** in one atomic action (often via an outbox pattern). This guarantees that even if a failure occurs right after the database write, the event of that price change is not lost – it can be retried or replayed. The **Outbox** ensures that each event is delivered to the Event Bus exactly once, so subscribers (like the Query Service or external integrators) will eventually receive it. Additionally, the use of **Circuit Breakers** around calls to external systems (Channel Mgt System) means if the external system is down, the HPS will not crash or hang trying to reach it. Instead, the circuit breaker opens and the Price Service can log the issue or queue the message for later, thus containing the failure. The design also mentions **bulk operation handling** – if a user makes many price changes at once (say uploading a new price sheet), the system can handle them in a batch transaction to maintain consistency and then emit a series of events, rather than doing each independently (which reduces the risk of partial failures in multi-step operations). All these tactics (atomic event recording, guaranteed messaging, failure isolation) work together to satisfy the reliability scenario QA-2: we can expect no lost or incomplete price updates even in adverse conditions.

* **QA-3 (Availability – 99.9% uptime):** High availability is primarily tackled through **redundancy and failover** in the deployment architecture. The system is designed to run in multiple availability zones or regions, especially the Price Query Service which is critical for uptime. A global load balancer can route requests to the healthiest region. If an entire region goes down, the system automatically fails over to a secondary region, thereby avoiding prolonged downtime. The architecture also likely employs redundant instances for each microservice (e.g. running several instances of each service in a cluster) so that even if one instance crashes, others continue serving. **Stateful components** like databases might use managed cloud services that offer replication and quick failover capabilities (though the document specifically highlights multi-region deployment as a strategy, acknowledging potential complexity with data consistency). Moreover, the system’s monitoring (QA-8) and health checks will detect outages quickly and can trigger automated recovery scripts or alerts. In sum, the **multi-region, multi-instance deployment strategy** is the key design decision to fulfill the 99.9% uptime requirement, ensuring that there is no single point of failure in production.

* **QA-4 (Scalability – up to 1,000,000 queries/day):** Scalability is achieved by making the system horizontally scalable at each tier. For handling a high number of read requests (price queries), the architecture relies on being able to **spin up additional Price Query Service instances** behind the API Gateway as load increases. Because the Query Service is stateless (queries hit either the cache or the database and do not depend on session state), load can be distributed easily. The introduction of a **Distributed Cache** cluster means that many repeated queries (e.g. the same hotel/date being looked up by multiple users) will be served from memory, which both speeds up responses and reduces database CPU usage. The database for queries can also scale (for example, using read replicas or a clustering technology) to handle more reads. On the write side, scalability is less of a concern since price changes are far less frequent than reads; nonetheless, separating the write service means it can also be scaled or optimized independently if needed. The design also contemplates **rate limiting** at the gateway – this prevents any one client from overloading the system and effectively smooths the load. This is a trade-off favoring system stability over serving every single request, but it’s a common decision to meet overall scalability and QoS targets. Overall, the architecture’s decision to decouple read vs. write and to allow horizontal scaling on the read side gives it the capacity to meet the 1,000,000 queries/day load without a significant performance penalty.

* **QA-5 (Security – authenticated & authorized access):** Security is woven through multiple layers of the design. The keystone decision is integrating with the **cloud’s Identity Service using OAuth2/OIDC** for authentication (as noted for HPS-1). This provides robust, vetted security for user login (including support for things like multi-factor auth if the IdP offers it). Once authenticated, the system issues a **JWT token** that encodes the user’s identity and roles, which every request must carry. The API Gateway is configured to validate these tokens on incoming requests and to enforce **authorization rules**: it checks if the user’s role permits the requested operation (for example, only an admin role token can access the “manage users” endpoints). This centralized enforcement means all microservices are protected by a consistent policy (they don’t each have to implement token verification logic, the gateway and a shared auth library handle it). The design also includes **Security Audit Logging** – recording all login attempts, permission changes, and possibly administrative actions for compliance and intrusion detection. These logs can be analyzed to spot unauthorized access attempts or to trace what happened in security-related incidents. By relying on standard protocols and centralizing critical security functions, the architecture addresses confidentiality and integrity concerns systematically. One additional design decision is that the Web front-end uses a **multi-platform responsive design** (per **CON-1**), which is a usability concern but also ties into security by ensuring there’s a single well-tested client interface for all devices (reducing potential inconsistencies that could lead to security issues on less common platforms).

* **QA-6 (Modifiability – adding new interfaces):** The ability to add a new service interface (like a gRPC API for price queries) without touching core logic is enabled by an **architectural layering and adapter pattern**. The system was designed with a **protocol-agnostic service layer** – for instance, in the Price Query Service, there is a mediator that sits between the external controllers (REST, gRPC) and the internal logic. To support gRPC, the team decided to use **Protocol Buffers** to define the service contracts, which can be used for both REST (with JSON mapping) and gRPC endpoints. Essentially, adding gRPC support would involve writing a new gRPC controller that converts gRPC calls to the internal service calls, very much like the REST controller already does, and because the data models and operations are defined independently of HTTP/REST specifics, this is straightforward. The **API Gateway** can also help by routing gRPC traffic to the appropriate service or even doing protocol translation if necessary. The modifiability here is a direct outcome of designing for loose coupling: by not baking any protocol assumptions into the core of the services, the system can evolve its interfaces (e.g. support GraphQL or gRPC concurrently with REST) with minimal changes. The use of a **layered architecture with clear separation** (or even a variant of hexagonal architecture for the services) is a conscious design decision to meet QA-6.

* **QA-7 (Deployability – environment portability):** To move the system through development, integration, staging, and production without code changes, the architecture relies on **containerization and automation**. The team uses container images (e.g. Docker) for all services, meaning the same artifact that runs in dev can run in prod. Differences in configuration (like database URLs, API keys, etc.) are externalized in config files or environment variables, managed by a config service or through container orchestration. The design explicitly calls for **Infrastructure as Code (IaC)** – scripts or templates that define cloud resources for each environment – to ensure each environment is set up consistently. Additionally, a **CI/CD pipeline** (Continuous Integration/Continuous Deployment) is established to automate the promotion of builds through these environments. For example, once code is merged, the pipeline might automatically run tests, build containers, deploy to an integration environment, run further tests, then move to staging, etc., up to production. This automation and consistency remove the need for manual tweaks or code forks for different environments. The outcome is that developers can focus on code, and when it’s time to release or test in a new environment, they can be confident the system will run the same way (minus configuration differences) as it did in previous ones – fulfilling the deployability requirement.

* **QA-8 (Monitorability – measuring performance & reliability):** The design incorporates a comprehensive **monitoring infrastructure** to ensure all important metrics can be collected. The use of **OpenTelemetry** across all services standardizes how metrics (like request latency, throughput), traces (end-to-end request flows), and logs are emitted. The system deploys components such as metric collectors, a time-series database for metrics, and possibly a distributed tracing backend to gather this telemetry. For the specific scenario (monitoring price publication performance and reliability), this means every price change event and its outcome (published to all subscribers or any failures) can be instrumented and tracked. The Admin Portal includes **monitoring dashboards** where operations staff or administrators can visualize these metrics in real time. Alerts can be set up for conditions like “price publication queue length is growing” or “event throughput is degraded”, which would notify operators of potential reliability issues. The architecture also mentions using **structured logging with correlation IDs** – when a price change is processed, all log entries from the Price Service to the Event Bus to the Query Service can be tied together, making it easy to trace a single transaction through the system. By investing in a standardized, end-to-end monitoring solution, the design ensures that *100% of relevant metrics* (per QA-8) can indeed be collected and analyzed, which is crucial for maintaining performance and reliability in production.

* **QA-9 (Testability – independent testing of system components):** Testability is addressed by designing the system to be **modular and decoupled**, and by using tooling to simulate external dependencies. Each microservice has a well-defined API and can be deployed (or tested) on its own, which is ideal for unit and integration testing of that service. For full system integration tests (end-to-end), the architecture avoids tight coupling to real external systems (like PMS, CMS) by using either stub implementations or containerized versions of those systems in a test environment. The documentation specifically calls out using **TestContainers**, a technology that allows spinning up lightweight throwaway instances of databases or other services for integration tests. For example, an integration test might bring up the Pricing service, a test database, and a mock Channel Management Service in containers, then execute a series of price changes and queries to verify everything works without needing access to a live CMS. Furthermore, the architecture’s emphasis on **contract testing** between services means each service pair (producer-consumer) can be validated against each other’s expected interface, catching incompatibilities early. This reduces reliance on large end-to-end tests and improves confidence that services can evolve independently without breaking others. All of these decisions ensure that the system can be tested thoroughly at multiple levels (unit, service integration, system integration) **without being blocked by unavailable external systems** – fulfilling the QA-9 scenario.

## **Potential Issues or Gaps**

While the proposed design addresses each requirement, there are some potential issues and gaps to consider:

* **Complexity and Over-Engineering:** The architecture employs numerous modern patterns (microservices, CQRS, event sourcing, outbox, DDD, etc.) all at once. For a mid-sized organization (300 hotels) this design is quite elaborate. There is a **risk of over-engineering**, which could lead to longer development times or steeper learning curves for the team. For example, implementing event sourcing and a custom rule engine are non-trivial tasks. If the team is not experienced in these patterns, it could introduce bugs or delays. The **initial timeline is only 6 months (2 months for MVP)**, so the sheer number of moving parts might jeopardize timely delivery if not carefully managed.

* **Eventual Consistency Concerns:** The use of an asynchronous, event-driven approach means some operations are not immediate. After a price change, the updated price is published to the Query Service and external systems asynchronously. There is a small window in which a query might not yet see the latest price. This *eventual consistency* might be acceptable (and is common in CQRS designs), but it’s a behavioral change from a traditional immediate-update system. If users expect to see their changes reflected *instantaneously* on queries, this needs to be managed (perhaps via the UI, by optimistically showing the new price). In practice the delay is likely very short, but it is a gap compared to strict consistency.

* **Operational Overhead:** Running a **multi-region deployment** with multiple microservices, each with its own database, event bus, cache cluster, etc., demands strong DevOps practices. Issues like data synchronization between regions (for the Price Query DB) could be complex – the design mentions the benefits of multi-region, but also notes the potential consistency issues with a global database. The team will need to set up replication or other strategies to keep data aligned, which is non-trivial. Similarly, the monitoring and CI/CD infrastructure (while very helpful) introduces additional components that must be maintained (telemetry systems, CI runners, etc.). There’s a risk that the **operational overhead** of this architecture (infrastructure as code, continuous deployment, container orchestration, etc.) is significant. If the team lacks experience in these, it could lead to configuration mistakes or reliability issues initially.

* **Performance Edge Cases:** Meeting the performance target of 100 ms for price updates assumes the computations are in-memory and efficient. This should hold true for typical cases (a single hotel’s rates calculation). However, if a user were to change prices for a large range of dates or a very large hotel with many room types, we need to be sure the system can still compute all derived rates in time. Caching helps, but if the data isn’t in cache or if rules are complex, it might approach the limit. There is no explicit mention of load testing results yet – achieving “\<100 ms” will require tuning. It’s something to verify during implementation (the design at least acknowledges this by tying the requirement to the Price Calc Engine component).

* **Rule Engine Complexity:** The custom rule engine for HPS-5, while valuable, adds a subsystem that must be developed and tested. There’s a gap in specifying whether they plan to build this from scratch or integrate an existing library. A homegrown rule engine might lack the maturity of off-the-shelf solutions and could become a maintenance burden if not done carefully. Non-technical users are expected to modify rules; the UI and UX for that need to be well thought out (this is not detailed in the architecture). If the rule engine’s interface isn’t user-friendly, one of the goals of HPS-5 (empowering admins to define business rules) might not be fully realized.

* **User Management Limitations:** The design uses RBAC with roles and central permission management, which is fine for now (since HPS-6 is about changing user permissions). However, they explicitly decided against more fine-grained **ABAC** (Attribute-Based Access Control) citing complexity. In the future, if the organization needs more nuanced access control (for example, permissions based on properties of hotels or dynamic conditions), the current RBAC scheme might prove too static. There’s no immediate gap since current requirements are satisfied, but it’s something to monitor as the system grows. Also, integration with the external identity system means any limitations of that system (for example, if it doesn’t easily support certain user metadata or groupings) could constrain the HPS’s user management. This is more of a forward-looking consideration.

* **Dependency on External Systems:** The new HPS will rely on the cloud identity service and still interacts with other systems (PMS, CMS, etc.) via REST. While decoupling is improved, any prolonged outage or performance issue in those external systems could impact the HPS’s functionality (e.g. if CMS is down, price changes queue up). The architecture mitigates a lot of this via circuit breakers and asynchronous queues, but there is still a dependency in real terms. Also, using a cloud identity service means if that service experiences downtime, users might not be able to log in to HPS. These are acknowledged trade-offs but worth noting as operational gaps to plan for (perhaps by having fallback procedures or clear communication when externals are down).

In summary, none of these issues are outright design flaws; rather, they are areas to watch during development and deployment. The architecture is comprehensive, but that itself demands careful implementation and might require deferring some complexity until the team is ready (for example, implementing multi-region deployment after initial release, if not immediately needed).

## **Trade-offs and Risks**

The architecture’s design decisions come with inherent trade-offs and risks, which are important to evaluate:

* **Microservices vs. Simplicity:** By splitting the system into many services (Auth, Hotel, Rate, Price, Query, etc.), the design achieves decoupling, independent deployability, and specialization of components. The trade-off is **increased complexity in integration** – inter-service communication, error handling, and data consistency become more complex than in a monolithic system. The risk here is mis-coordination between services or partial failures. However, the use of an Event Bus and clear service boundaries mitigates some of this by making interactions asynchronous and resilient. It shifts the complexity into managing the event-driven aspects (e.g. ensuring the Query service correctly handles out-of-order events or duplicate events, etc.).

* **Event-Driven CQRS Trade-offs:** The CQRS \+ event sourcing approach greatly improves read scalability and write throughput, and provides an audit log, but it introduces **eventual consistency** and a lot of infrastructural code (event handling, syncing read models) that wouldn’t exist in a simpler CRUD design. The team has accepted the eventual consistency trade-off, presumably because the business can tolerate a short delay on price visibility in queries. They gain scalability and maintainability (separating read and write concerns) at the cost of having to manage an event pipeline. A risk is that if the event processing falls behind (say the event bus experiences lag), the read model might become stale relative to recent writes. The architecture will need robust monitoring on that (which QA-8 covers) and perhaps back-pressure or alerting if such a lag occurs.

* **Performance vs. Consistency:** The design choices for performance (caching, asynchronous updates) prioritize speed over absolute consistency. For example, caching can lead to stale data if not properly invalidated, and asynchronous external updates mean the system is not blocking to confirm every external system got the update. The benefit is very fast responses and a loosely coupled system; the downside is complexity in ensuring consistency and the risk of serving stale data in rare cases. The designers tried to balance this by using event sourcing (so the source of truth is the event log) and by having the Query service update from that log to keep caches in sync. It’s a classic trade-off in distributed systems, and here the choice leans toward performance and reliability over immediate consistency.

* **Use of Cloud Services (Identity, etc.):** Offloading identity management to a cloud service (and hosting in the cloud) yields benefits: less to build in-house, and likely high reliability and compliance from the provider. The trade-off is **vendor dependency** – the system is tied to that cloud provider’s identity system and other cloud services. The risk is if the provider changes APIs, experiences outages, or increases costs, the HPS has limited control. Mitigating this, the system uses standard protocols (OAuth/OIDC) which makes it easier to swap identity providers if needed. Also, the “cloud-native” approach (per **CON-6**) is a strategic choice by the company, so the design aligns with that intentionally, accepting the trade-offs of cloud dependency for better scalability and maintenance.

* **Maintainability vs. Time-to-Market:** Many of the choices (like DDD, separate services for each domain area, avoiding technical debt with contract testing and feature flags) are aimed at long-term maintainability and agility. The trade-off is a possibly slower initial development, since the team must set up and adhere to these practices from the start. The risk is that the **initial development may take longer** or be more complex than a quick-and-dirty approach. Given the six-month deadline, there is a slight tension here: the architecture is banking on robust practices to avoid debt, which is good for the long run, but the team must be disciplined to not fall behind schedule. The iteration plan prioritizes core functionality first (which is good), and presumably adds the polish (monitoring, testability, etc.) later, which is a way to balance this trade-off.

* **Technology Risks:** Introducing a rule engine, distributed caching, and an event bus means bringing in additional technologies or libraries (perhaps Apache Kafka for the event bus, Redis or similar for caching, etc.). Each of these has its own learning curve and pitfalls. For instance, using an event bus requires deciding on message serialization, partitioning, error handling (dead letter queues, etc.). The risk is that if any of these technologies are misconfigured or used incorrectly, it could cause system instability. The design tries to mitigate this by using well-understood patterns (outbox to ensure exactly-once, circuit breakers to handle failures gracefully). Still, the team will need expertise in these areas. There’s also a mention of possibly using **gRPC and Protocol Buffers** for modifiability – adopting those adds complexity (developers need to write .proto files, manage backward compatibility of messages, etc.). The benefit is a more flexible, high-performance interface, but they must ensure the team is trained for it, otherwise it could slow them down or result in misuse.

* **Timeline and Resource Risks:** As noted, fulfilling all quality attributes within 6 months is aggressive. The plan breaks it down into iterations focusing on different aspects. A risk is that some quality aspects (like multi-region deployment or comprehensive monitoring) might not be fully realized by the initial release. The trade-off choice here might be that they implement the hooks for these features but perhaps not deploy multi-region from day one (maybe they start in one region but keep the option open). If the team runs short on time, they might also postpone some lower-priority features (for example, full test container integration or fancy monitoring dashboards) in favor of delivering the core functionality. This is more of a project management trade-off than an architectural one, but it’s driven by the architecture’s breadth – covering everything could strain the schedule, so practical scoping decisions may be needed.

* **User Experience vs. Architectural Purity:** The architecture is very back-end centric in its description. One subtle trade-off is how much logic is in the front-end (browser) vs. the back-end. The requirement was to support web browsers on multiple platforms (CON-1), which they addressed by using a web app with responsive design. The design does not opt for native apps (which is a conscious decision to avoid extra development effort). By doing so, they ensure one codebase for UI, but rely on the web for everything. This is likely a good trade-off given the business context. The risk is minimal here, but if certain complex operations (like price simulation) need a lot of user interaction, the front-end (Angular presumably, given CRN-2) has to be robust. The architecture doesn’t detail the front-end much beyond stating its existence and responsibilities. We assume the team’s familiarity with Angular mitigates risk on the UI side.

Overall, the architecture’s trade-offs favor **decoupling, reliability, and future-proofing at the cost of added complexity and high initial effort**. These choices seem justified by the drivers (they explicitly wanted to solve reliability, performance, and maintainability issues of the old system, and to align with new enterprise principles of decoupling). The key risks lie in execution: if the team can handle the complexity, the design will pay off; if not, they might have to simplify some aspects on the fly.

## **Conclusion**

Considering all factors, the proposed architecture for the AD\&D Hotels Pricing System appears **comprehensive and well-aligned with the requirements**. It explicitly addresses each major user story and quality attribute, using modern architectural patterns to satisfy business needs (like flexible price management and fast queries) as well as critical system qualities (performance, reliability, security, etc.). The design shows a strong awareness of the challenges in the previous system (tight coupling, propagation of failures) and directly counters them with a decoupled, resilient approach (microservices with an event bus and circuit breakers).

In terms of **functional adequacy**, the architecture covers all use cases: from user login/security, through core pricing operations, to ancillary admin features (hotel/rate/user management). Each feature has a clear place in the design (e.g., separate services for each admin domain, specialized UIs for admin vs. regular users). There do not appear to be missing features – even “simulate price change” is accounted for in the Price Service design. This thorough mapping of features to components indicates the architecture can deliver the required functionality.

For **quality attributes**, the design doesn’t just meet the bare minimum; it often provides a robust solution that anticipates growth. For example, to achieve performance and scalability, it doesn’t just assume a beefy server – it implements CQRS, caching, and horizontal scaling which will allow the system to scale by design rather than brute force. Security is handled via industry standards, which is a sound decision. Modifiability is built in via layering and protocol abstraction, meaning the system can evolve (support new APIs, integrate new channels) with less pain. This is important given the business’s likely future needs (e.g. perhaps integrating with more online travel agencies or providing gRPC APIs to partners).

The **decoupled, event-driven nature** of the system significantly improves reliability and maintainability. It means each service can be developed and tested on its own, and a failure in one (say the reporting system) will not easily cascade to others – a failure scenario the old system suffered from. The use of cloud infrastructure and DevOps practices (continuous deployment, IaC) also suggests that the system will be easier to deploy and manage than the legacy system, aligning with the company’s move to Agile/DevOps culture.

That said, the architecture is **ambitious**. Its adequacy assumes that the team can implement and integrate all these parts correctly. If they can, the result will be a state-of-the-art system that not only meets current needs but is poised to handle future requirements and expansions. If they struggle, the risk is not with the design itself but with delivering it on time and with all features fully realized. However, given the drivers and constraints, it’s understandable why the architects chose this route: the system needed a clean break from the past and a foundation that will serve for years to come.

In conclusion, the proposed design is **adequate and even forward-looking** for the problem at hand – it addresses the pains of the old system and the user stories of the new system with carefully chosen architectural tactics. The design should be able to fulfill AD\&D Hotels’ needs, provided that the implementation is executed with the same rigor as the design. The emphasis on avoiding technical debt and ensuring testability and monitorability from the start will help maintain the system’s adequacy in production. The true test will be in execution, but architecturally, the solution is solid and satisfies the specified drivers.

## **Recommendations**

To further strengthen the design and mitigate the identified risks, we offer the following recommendations:

* **Incremental Implementation:** Phase the introduction of complex patterns in a controlled manner. For example, start with the core pricing functionality (as planned for the MVP) and perhaps initially use a simpler approach for event handling if needed (even a straightforward messaging queue) before fully implementing the outbox \+ event sourcing mechanism. Ensure that each piece (event bus, caching layer, rule engine, etc.) is proven in isolation with prototypes. This can prevent the team from being overwhelmed and allow learning and adjustments early.

* **Thorough Performance and Failure Testing:** Before going live, conduct rigorous testing for the critical scenarios. This includes load testing price calculations to verify the 100 ms target under realistic conditions and high concurrency. Also test failure scenarios – e.g., what happens if the Channel Management System is down for an hour? Does the outbox successfully retry and recover once it’s up? What if the event bus is down or slow? By simulating these, the team can fine-tune timeout, retry, and circuit-breaker settings to ensure reliability (QA-2) is truly achieved in practice.

* **Monitoring and Alerting Policies:** Since the design heavily leverages monitoring (QA-8), the team should define clear metrics and alert thresholds for all critical services. For instance, set up alerts for event backlog size, slow query responses, high error rates on any service, or authentication failures. Using the OpenTelemetry data, configure dashboards for these metrics from day one. This will help catch issues early in testing and ensure that once in production, any deviation from expected behavior is caught and addressed before it impacts users. Essentially, make operational readiness (alerts, runbooks for what to do on certain alerts) part of the development process.

* **Review Scope vs. Timeline:** Keep an eye on the project timeline (6 months) and be prepared to prioritize or defer features. The iteration plan already orders things by priority, which is good. As an additional measure, identify any “nice-to-have” aspects in the architecture that could be simplified if deadlines loom. For example, if multi-region deployment (QA-3) is complex to do immediately, consider launching in a single region but design for expansion. Or if full ABAC is not needed now (and it isn’t), stick to RBAC but document the potential need to revisit it in the future. In other words, consciously manage technical debt by possibly postponing certain lower-priority capabilities, but keeping the design flexible to add them later. The architecture already includes feature flags and contract testing to avoid debt – use those practices to deliver a simpler initial version that can evolve.

* **Leverage Existing Tools/Services:** Where possible, use well-established technologies to implement the architectural patterns. For instance, use a proven event streaming platform (like Kafka or AWS SNS/SQS) for the Event Bus rather than building a custom one. Use a library or engine for the rule engine if one fits (there are open-source rule engines that could be integrated). This reduces risk by relying on battle-tested components. The team should also follow reference architectures from the cloud provider for multi-region deployments, CI/CD pipelines, etc., to avoid reinventing wheels and to adhere to best practices.

* **Team Training and Knowledge Sharing:** Ensure the development team is up-to-speed with the technologies and patterns being used. It might be worthwhile to do workshops on event-driven microservices, practice with TestContainers, or training on the OpenTelemetry stack. The architecture leans on the team’s knowledge of Java and Angular (per **CRN-2**), which is good, but some introduced concepts (like distributed tracing, gRPC, Protocol Buffers) may be new. Investing time in training will pay off in a smoother implementation and fewer mistakes, ultimately safeguarding the architecture’s integrity.

By following these recommendations, AD\&D Hotels can increase the likelihood that the ambitious architecture is implemented successfully and delivers the expected benefits. The architecture is strong; these steps will help ensure the project execution is equally strong, resulting in a resilient, efficient Hotel Pricing System that meets all its drivers.
